# ğŸ›¡ï¸ Offensive Comment Analyzer

This project is a Python-based tool that detects and categorizes offensive content from user comments using the **Google Gemini API** and **profanity filtering**. It helps in moderating user-generated content by flagging hate speech, toxicity, profanity, and harassment, along with visual summaries.

---

## ğŸš€ Features

- ğŸ” Detects whether a comment is offensive or not  
- ğŸ·ï¸ Categorizes offense type: `hate speech`, `toxicity`, `profanity`, `harassment`, or `none`  
- ğŸ¯ Assigns severity on a scale of 1â€“5  
- ğŸ§  Uses Google Gemini API for AI-powered analysis  
- ğŸ§¹ Uses `better_profanity` for keyword-based filtering  
- ğŸ“Š Visualizes offense type distribution (Bar + Pie charts)  
- ğŸ“„ Outputs a detailed CSV report and summary  

---
## Demo ->

## ğŸ“‚ Input

CSV file with the following columns:

csv
comment_id,comment_text
ğŸ“¤ Output
A new CSV file with added columns:

is_offensive

offense_type

explanation

severity

Bar chart: offense_bar_chart.png

Pie chart: offense_pie_chart.png

Console output summary

## âš™ï¸ Installation
Install required libraries using:


pip install requests matplotlib better_profanity
ğŸ”‘ API Key
Replace this line in the script:

api_key = "API_KEY"
with your actual API key from Google AI Studio.

## â–¶ï¸ How to Run

python script.py input.csv output.csv
Example:

python detect_offensive_comments.py comments.csv results.csv
Make sure input.csv exists in the correct format.

## ğŸ§ª What It Does
Loads all comments from the input CSV

Checks for basic profanity using better_profanity

Sends the comments to Gemini API for detailed analysis

Updates each comment with analysis results

Saves results in a new CSV

Displays a summary and charts showing offense breakdown
